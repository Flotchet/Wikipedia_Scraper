{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rru15J6cT0aF"
      },
      "source": [
        "# My code is not cleaned\n",
        "# Your first scraper\n",
        "In this project, we will guide you step by step through the process of:\n",
        "\n",
        "1. creating a self-contained development environment.\n",
        "1. retrieving some information from an API (a website for computers)\n",
        "2. leveraging it to scrape a website that does not provide an API\n",
        "3. saving the output for later processing\n",
        "\n",
        "Here we query an API for a list of countries and their past leaders. We then extract and sanitize their short bio from Wikipedia. Finally, we save the data to disk.\n",
        "\n",
        "This task is often the first (coding) step of a datascience project and you will often come back to it in the future.\n",
        "\n",
        "You will study topics such as *scraping*, *data structures*, *regular expressions*, *concurrency* and *file handling*. We will point out useful resources at the appropriate time. \n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a clean environment\n",
        "\n",
        "Use the [`venv`](https://docs.python.org/3/library/venv.html) command to create a new environment called `wikipedia_scraper`.\n",
        "\n",
        "You will find more info about virtual environments [here](../2.python/1.python_fundamentals/02.Package-Managers/00.Package_Managers.md).\n",
        "\n",
        "Activate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48qvKH56a1Wy"
      },
      "source": [
        "## API Scraping\n",
        "\n",
        "### A simple API query\n",
        "You will start with the basics: how to do a simple request to an [API endpoint](../../2.python/2.python_advanced/05.Scraping/5.apis.ipynb).\n",
        "\n",
        "You will use the [requests](https://requests.readthedocs.io/en/latest/) external library through the `import` keyword.\n",
        "\n",
        "NOTE: external libraries need to be installed first. Check their documentation.\n",
        "\n",
        "Check the [Quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/) section of the Requests documentation to:\n",
        "1. use the `get()` method to connect to this endpoint: https://country-leaders.onrender.com/status\n",
        "2. check if the `status_code` is equal to 200, which means OK.\n",
        "    * if OK, `print()` the `text`` of the response.\n",
        "    * if not, `print()` the `status_code`. \n",
        "\n",
        "Here is the signification of [HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check the status_code using a condition and print appropriate messages (4 lines)\n",
        "def status_to_text(status_code : int) -> str:\n",
        "    status_dict = {\n",
        "        # infos response\n",
        "        100: \"Continue\",\n",
        "        101: \"Switching Protocols\",\n",
        "        102: \"Processing\",\n",
        "        103: \"Early Hints\",\n",
        "        # success response\n",
        "        200: \"OK\",\n",
        "        201: \"Created\",\n",
        "        202: \"Accepted\",\n",
        "        203: \"Non-Authoritative Information\",\n",
        "        204: \"No Content\",\n",
        "        205: \"Reset Content\",\n",
        "        206: \"Partial Content\",\n",
        "        207: \"Multi-Status\",\n",
        "        208: \"Already Reported\",\n",
        "        226: \"IM Used\",\n",
        "        # redirection response\n",
        "        300: \"Multiple Choices\",\n",
        "        301: \"Moved Permanently\",\n",
        "        302: \"Found\",\n",
        "        303: \"See Other\",\n",
        "        304: \"Not Modified\",\n",
        "        305: \"Use Proxy\",\n",
        "        306: \"Switch Proxy\",\n",
        "        307: \"Temporary Redirect\",\n",
        "        308: \"Permanent Redirect\",\n",
        "        # client error response\n",
        "        400: \"Bad Request\",\n",
        "        401: \"Unauthorized\",\n",
        "        402: \"Payment Required\",\n",
        "        403: \"Forbidden\",\n",
        "        404: \"Not Found\",\n",
        "        405: \"Method Not Allowed\",\n",
        "        406: \"Not Acceptable\",\n",
        "        407: \"Proxy Authentication Required\",\n",
        "        408: \"Request Timeout\",\n",
        "        409: \"Conflict\",\n",
        "        410: \"Gone\",\n",
        "        411: \"Length Required\",\n",
        "        412: \"Precondition Failed\",\n",
        "        413: \"Payload Too Large\",\n",
        "        414: \"URI Too Long\",\n",
        "        415: \"Unsupported Media Type\",\n",
        "        416: \"Range Not Satisfiable\",\n",
        "        417: \"Expectation Failed\",\n",
        "        418: \"I'm a teapot\",\n",
        "        421: \"Misdirected Request\",\n",
        "        422: \"Unprocessable Entity\",\n",
        "        423: \"Locked\",\n",
        "        424: \"Failed Dependency\",\n",
        "        425: \"Too Early\",\n",
        "        426: \"Upgrade Required\",\n",
        "        428: \"Precondition Required\",\n",
        "        429: \"Too Many Requests\",\n",
        "        431: \"Request Header Fields Too Large\",\n",
        "        451: \"Unavailable For Legal Reasons\",\n",
        "        # server error response\n",
        "        500: \"Internal Server Error\",\n",
        "        501: \"Not Implemented\",\n",
        "        502: \"Bad Gateway\",\n",
        "        503: \"Service Unavailable\",\n",
        "        504: \"Gateway Timeout\",\n",
        "        505: \"HTTP Version Not Supported\",\n",
        "        506: \"Variant Also Negotiates\",\n",
        "        507: \"Insufficient Storage\",\n",
        "        508: \"Loop Detected\",\n",
        "        510: \"Not Extended\",\n",
        "        511: \"Network Authentication Required\"\n",
        "    }\n",
        "    # find if the status code is in the keys\n",
        "    if status_code in status_dict.keys():\n",
        "        return str(status_code) + \": \" + status_dict[status_code]\n",
        "    else:\n",
        "        return str(status_code) + \": Unknown or unofficial status code\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "9baoMWgIcK3E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://country-leaders.onrender.com/status 200: OK\n",
            "b'\"Alive\"'\n"
          ]
        }
      ],
      "source": [
        "# import the requests library (1 line)\n",
        "import requests\n",
        "\n",
        "# assign the root url (without /status) to the root_url variable for ease of reference (1 line)\n",
        "root_url = \"https://country-leaders.onrender.com\"\n",
        "\n",
        "# assign the /status endpoint to another variable called status_url (1 line)\n",
        "status_url = \"/status\"\n",
        "\n",
        "# query the /status endpoint using the get() method and store it in the req variable (1 line)\n",
        "requ = requests.get(root_url + status_url)\n",
        "   \n",
        "print(root_url + status_url + \" \" + status_to_text(requ.status_code))\n",
        "# print the content of the webpage if stautus is 200\n",
        "if requ.status_code == 200:\n",
        "    print(requ.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL72ovJUcZTA"
      },
      "source": [
        "### Dealing with JSON\n",
        "\n",
        "[JSON](https://quickref.me/json) is the preferred format to deal with data over the web. You cannot avoid it so you would better get acquainted.\n",
        "\n",
        "Connect to another endpoint called `/countries` but this time the API will return data in the JSON format. Check what to do in the Requests [Quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Y0DLiYCWcg5W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://country-leaders.onrender.com/countries 403: Forbidden\n"
          ]
        }
      ],
      "source": [
        "# Set the country_url variable (1 line)\n",
        "country_url = \"/countries\"\n",
        "\n",
        "# query the /countries endpoint using the get() method and store it in the req variable (1 line)\n",
        "requ =  requests.get(root_url + country_url)\n",
        "\n",
        "# Get the JSON content and store it in the countries variable (1 line)\n",
        "countries = requ.json()\n",
        "\n",
        "# display the request's status code and the countries variable (1 line)\n",
        "print(root_url + country_url + \" \" + status_to_text(requ.status_code))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x25JA6vaRBi"
      },
      "source": [
        "### Cookies anyone?\n",
        "\n",
        "It looks like the access to this API is restricted...\n",
        "Query the `/cookie` endpoint and extract the appropriate field to access your cookie.\n",
        "\n",
        "You will need to use this cookie in each of the following API requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "dTDwpN9Q3nk_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<RequestsCookieJar[<Cookie user_cookie=b36cbbd6-24a5-4aad-97ea-fe374be28bb0 for country-leaders.onrender.com/>]>\n"
          ]
        }
      ],
      "source": [
        "# Set the session\n",
        "session = requests.Session()\n",
        "\n",
        "# Set the cookie_url variable (1 line)\n",
        "cookie_url = \"/cookie\"\n",
        "\n",
        "# Query the enpoint, set the cookies variable and display it (3 lines)\n",
        "requ = requests.get(root_url + cookie_url)\n",
        "cookies = requ.cookies\n",
        "print(cookies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHBNaFJo2M9e"
      },
      "source": [
        "Try to query the countries endpoint using the cookie, save the output and print it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "9Y63sTXY7ppT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://country-leaders.onrender.com/countries 200: OK\n",
            "['us', 'ma', 'be', 'fr', 'ru']\n"
          ]
        }
      ],
      "source": [
        "# query the /countries endpoint, assign the output to the countries variable (1 line) \n",
        "requ = requests.get(root_url + country_url, cookies=cookies)\n",
        "\n",
        "# and check the status code and the countries variable's content (3 lines)\n",
        "print(root_url + country_url + \" \" + status_to_text(requ.status_code))  \n",
        "print(requ.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3HmuGTT9lU1"
      },
      "source": [
        "Chances are the cookie has expired... Thanksfully, you got a nice error message. For now, simply execute the last 2 cells quickly so you get a result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egv40GBV8rSH"
      },
      "source": [
        "### Getting the actual data from the API\n",
        "\n",
        "Query the `/leaders` endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ZwLFqcBA8PaD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://country-leaders.onrender.com/leaders 404: Not Found\n",
            "b'{\"message\":\"Please specify a country\"}'\n"
          ]
        }
      ],
      "source": [
        "# query the /leaders endpoint, assign the output to the leaders variable (1 line)using the cookie\n",
        "requ = requests.get(root_url + \"/leaders\", cookies=cookies)\n",
        "\n",
        "# and check its content (4 lines)\n",
        "print(root_url + \"/leaders\" + \" \" + status_to_text(requ.status_code))\n",
        "print(requ.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QyM7vWBAlY4"
      },
      "source": [
        "It looks like this endpoint requires additional information in order to return its result. Check the API [*documentation*](https://country-leaders.onrender.com/docs) in your web browser.\n",
        "\n",
        "Change the query to accept *parameters*. You should know where to find help by now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "gIEFBhBeAkzf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://country-leaders.onrender.com/leaders 200: OK\n",
            "{'id': 'Q36023', 'first_name': 'John', 'last_name': 'Coolidge', 'birth_date': '1872-07-04', 'death_date': '1933-01-05', 'place_of_birth': 'Plymouth Notch', 'wikipedia_url': 'https://en.wikipedia.org/wiki/Calvin_Coolidge', 'start_mandate': '1923-08-02', 'end_mandate': '1929-03-04'}\n"
          ]
        }
      ],
      "source": [
        "# Set the leaders_url variable (1 line)\n",
        "leaders_url = \"/leaders\"\n",
        "\n",
        "# query the /leaders endpoint using cookies and parameters (take any country in countries)\n",
        "# assign the output to the leaders variable (3 lines)\n",
        "requ = requests.get(root_url + cookie_url)\n",
        "\n",
        "cookies = requ.cookies\n",
        "requ1 = requests.get(root_url + country_url, cookies=cookies)\n",
        "\n",
        "countries = requ1.json()\n",
        "requ2 = requests.get(root_url + leaders_url, cookies=cookies, params={\"country\": countries[0]})\n",
        "\n",
        "# check the result (3 lines) \n",
        "print(root_url + leaders_url + \" \" + status_to_text(requ2.status_code))\n",
        "print(requ2.json()[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW3k5uquCirA"
      },
      "source": [
        "### A sneak peak at the data (finally)\n",
        "\n",
        "Look inside a few examples. Notice the dictionary keys available for each entry. You have your first example of *structured data*. This data was sanitized for your benefit, meaning it is readily exploitable without modification.\n",
        "\n",
        "You will also notice there is a Wikipedia link for each entry. You will need to extract additional information there. This will be a case of *semi-structured* data.\n",
        "\n",
        "The /countries endpoint returns a `list` of several country codes.\n",
        "\n",
        "You need to loop through this list and query the /leaders endpoint for each one. Save each entry in a dictionary called `leaders_per_country` like this one: \n",
        "`{'fr':[{leader0}, {leader1},...],\n",
        "'en':[{leader0}, {leader1},...],\n",
        "...\n",
        "}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "H8EdK2S9rvPJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'Q6279', 'first_name': 'Joe', 'last_name': 'Biden', 'birth_date': '1942-11-20', 'death_date': None, 'place_of_birth': \"St. Mary's Hospital\", 'wikipedia_url': 'https://en.wikipedia.org/wiki/Joe_Biden', 'start_mandate': '2021-01-20', 'end_mandate': None}\n"
          ]
        }
      ],
      "source": [
        "# 5 lines\n",
        "requ = requests.get(root_url + cookie_url)\n",
        "\n",
        "cookies = requ.cookies\n",
        "requ1 = requests.get(root_url + country_url, cookies=cookies)\n",
        "\n",
        "countries = requ1.json()\n",
        "\n",
        "lead = [requests.get(root_url + leaders_url, cookies=cookies, params={\"country\": country}).json() for country in countries]\n",
        "leaders_dict = dict(map(lambda i,j : (i,j) , countries , lead)) \n",
        "\n",
        "print(leaders_dict[\"us\"][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsA2j7s_RMgy"
      },
      "source": [
        "It is finally time to create a `get_leaders()` function for the above code. You will build on it later-on. This function takes no parameter. Inside it, you will need to:\n",
        "1. define the urls\n",
        "2. get the cookies\n",
        "2. get the countries\n",
        "3. loop over them and save their leaders in a dictionary\n",
        "4. return the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "p6C7BgWQMxV2"
      },
      "outputs": [],
      "source": [
        "# 15 lines\n",
        "def get_leaders():\n",
        "\n",
        "    root_url = \"https://country-leaders.onrender.com\"\n",
        "    cookie_url = \"/cookie\"\n",
        "    country_url = \"/countries\"\n",
        "    leaders_url = \"/leaders\"\n",
        "\n",
        "    requ = requests.get(root_url + cookie_url)\n",
        "    cookies = requ.cookies\n",
        "\n",
        "    requ1 = requests.get(root_url + country_url, cookies=cookies)\n",
        "    countries = requ1.json()\n",
        "\n",
        "    lead = [requests.get(root_url + leaders_url, cookies=cookies, params={\"country\": country}).json() for country in countries]\n",
        "    leaders_dict = dict(map(lambda i,j : (i,j) , countries , lead)) \n",
        "    \n",
        "    return leaders_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I76-InoKuuV8"
      },
      "source": [
        "Test your function, save the result in the `leaders_per_country` dictionary and check its ouput."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "eXwd8o7Gu8yG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'Q6279', 'first_name': 'Joe', 'last_name': 'Biden', 'birth_date': '1942-11-20', 'death_date': None, 'place_of_birth': \"St. Mary's Hospital\", 'wikipedia_url': 'https://en.wikipedia.org/wiki/Joe_Biden', 'start_mandate': '2021-01-20', 'end_mandate': None}\n"
          ]
        }
      ],
      "source": [
        "# 2 lines\n",
        "leaders_per_country = get_leaders()\n",
        "print(leaders_per_country[\"us\"][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Fc1mHySn9g"
      },
      "source": [
        "## Extracting data from Wikipedia\n",
        "\n",
        "Query one of the leaders' Wikipedia urls and display its `text` (not JSON)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "cEKKqyTHr3fD"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n",
        "import requests\n",
        "macron_url = 'https://fr.wikipedia.org/wiki/Emmanuel_Macron'\n",
        "putin_url = \"https://ru.wikipedia.org/wiki/%D0%9F%D1%83%D1%82%D0%B8%D0%BD,_%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80_%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B8%D1%87\"\n",
        "req = requests.get(\"https://ru.wikipedia.org/wiki/%D0%9F%D1%83%D1%82%D0%B8%D0%BD,_%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80_%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B8%D1%87\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlsqjiTYr8sK"
      },
      "source": [
        "Ouch! You get the raw HTML code of the webpage. If you try to deal with it without tools, you will be there all night. Instead, use the [beautiful soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) *external* library. You will find more info about it [here](../../2.python/2.python_advanced/05.Scraping/1.beautifulsoup_basic.ipynb) and [here](../../2.python/2.python_advanced/05.Scraping/2.beautifulsoup_advanced.ipynb)\n",
        "\n",
        "Using the Quickstart section, start by importing the library and loading the output of your `get_text()` function.\n",
        "\n",
        "Use the `prettify()` function to take a look. You will start the actual parsing in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "h79ahwJvr7p-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Путин, Владимир Владимирович — Википедия\n"
          ]
        }
      ],
      "source": [
        "# 3 lines\n",
        "import bs4\n",
        "soup = bs4.BeautifulSoup(req.content, \"html.parser\")\n",
        "print(soup.find(\"title\").text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_first_paragraph(url):\n",
        "\n",
        "    req = requests.get(url)\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "\n",
        "    for paragraph in soup.find_all('p'):\n",
        "\n",
        "        if paragraph.find_all('b'):\n",
        "            \n",
        "            return paragraph.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Emmanuel Macron (/ɛmanɥɛl makʁɔ̃/[e] Écouter), né le 21 décembre 1977 à Amiens est un haut fonctionnaire et homme d'État français. Il est président de la République française depuis 2017.\\n\""
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_first_paragraph(url=macron_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsLjaig7_dY"
      },
      "source": [
        "That looks better but you need to extract the right part of the webpage: the text of the first paragraph.\n",
        "\n",
        "It is a bit tricky because Wikipedia pages slightly differ in structure from one language to the next. We cannot simply get the text for the first HTML paragraph.\n",
        "\n",
        "You will start by getting all the HTML paragraphs from the HTML source and saving them in the `paragraphs` variable.\n",
        "\n",
        "Use the documentation or google the appropriate keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Vs8HeBx19oyC"
      },
      "outputs": [],
      "source": [
        "# 2 lines\n",
        "\n",
        "def get_all_paragraphs(url):\n",
        "    \n",
        "    req = requests.get(url)\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "    return [paragraph.text for paragraph in soup.find_all('p')]\n",
        "\n",
        "paragraphs = get_all_paragraphs(url=macron_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tlaL3aM9zoo"
      },
      "source": [
        "If you try different urls, you might find that the paragraph you want may be at a different index each time.\n",
        "\n",
        "That is where you need to be clever and ask yourself what would be a reliable way to identify the right index ie. which string matches only the first paragraph whatever the language...\n",
        "\n",
        "Spend a good 30 minutes on the problem and brainstorm with your fellow learners. If you come out empty handed, ask your coach.\n",
        "\n",
        "1. Loop over the HTML paragraphs\n",
        "2. When you have identified the correct one\n",
        "  * store the [text](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#output) inside the `first_paragraph` variable\n",
        "  * exit the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "0DduDXaQALau"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Emmanuel Macron (/ɛmanɥɛl makʁɔ̃/[e] Écouter), né le 21 décembre 1977 à Amiens est un haut fonctionnaire et homme d'État français. Il est président de la République française depuis 2017.\n",
            "\n",
            "Влади́мир Влади́мирович Пу́тин (род. 7 октября 1952, Ленинград, СССР) — российский государственный и политический деятель. Действующий президент Российской Федерации, председатель Государственного Совета Российской Федерации и верховный главнокомандующий Вооружёнными силами Российской Федерации с 7 мая 2012 года. Ранее занимал должность президента с 7 мая 2000 года по 7 мая 2008 года, также в 1999—2000 и 2008—2012 годах занимал должность председателя правительства Российской Федерации. Кандидат экономических наук (1997).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 6 lines\n",
        "# there 2 things always present in the first paragraph \n",
        "# 1. all the words of the title but not concistant since for exemple russian words can change accent for  some reason\n",
        "# 2. a text (the full title in bold)\n",
        "# so we can get the first paragraph by fiding the first paragraph of the \n",
        "# page that contains all the words of the title in bold\n",
        "\n",
        "import string\n",
        "def get_first_paragraph(url):\n",
        "    \n",
        "    req = requests.get(url)\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "    \n",
        "    title = soup.find('h1').text\n",
        "    title = title.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    title_words = title.split()\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        for bold in paragraph.find_all('b'):\n",
        "            return paragraph.text\n",
        "        \n",
        "print(get_first_paragraph(url=macron_url))\n",
        "print(get_first_paragraph(url=putin_url))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFe-1LlIHBGm"
      },
      "source": [
        "At this stage, you can create a function to maintain consistency in your code. We will give you its *skeleton*, you will copy the code you wrote and make it work inside a function.\n",
        "\n",
        "Don't forget to test your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "wQORoweDHARO"
      },
      "outputs": [],
      "source": [
        "# 10 lines\n",
        "import string\n",
        "def get_first_paragraph(url):\n",
        "    \n",
        "    req = requests.get(url)\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "    #rtitle = soup.find('h1').text\n",
        "    #remove punctuation character\n",
        "    #rtitle = title.translate(str.maketrans('', '', string.punctuation))\n",
        "    #rtitle_words = title.split()\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        for bold in paragraph.find_all('b'):\n",
        "            #if all(word in bold.text for word in title_words):\n",
        "            return paragraph.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtoM4dgsBVoD"
      },
      "source": [
        "### Regular expressions to the rescue\n",
        "\n",
        "Now that you have extracted the content of the first paragraph, the only thing that remains to finish your Wikipedia scraper is to sanitize the output.\n",
        "\n",
        "Indeed some Wikipedia references, HTML code, phonetic pronunciation etc. may linger. You might find *regular expressions* handy to get rid of them and obtain pristine text. You will find some useful documentation about regular expressions [here](../../2.python/2.python_advanced/03.Regex/regex.ipynb)\n",
        "\n",
        "Once you have one of your regex working online, try it in the cell below. \n",
        "\n",
        "Hints: \n",
        "* Check the `sub()` method documentation.\n",
        "* Make sure to test urls in different languages. Some may look good but other do not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "7DHEAb6oBUxd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(16, 45), match='(/ɛmanɥɛl makʁɔ̃/[e] Écouter)'>\n"
          ]
        }
      ],
      "source": [
        "# 4 lines\n",
        "import re\n",
        "# regular expression string to find things like this (/ɛmanɥɛl makʁɔ̃/[e] Écouter)\n",
        "regex = \"\\(/.+/\\[e\\].*\\)\"\n",
        "# compile the pattern\n",
        "pattern = re.compile(regex)\n",
        "# search\n",
        "result = re.search(pattern, get_first_paragraph(url=macron_url))\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QekqWs-4E0bK"
      },
      "source": [
        "Overwrite the `get_first_paragraph()` function by applying your regex to the first paragraph before returning it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "voT-jzd7FMOc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Emmanuel Macron, né le 21 décembre 1977 à Amiens est un haut fonctionnaire et homme d'État français. Il est président de la République française depuis 2017.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 10 lines\n",
        "def get_first_paragraph(url):\n",
        "    req = requests.get(url=url)\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "    regex = \"\\ \\(/.+/\\[e\\].*\\)\"\n",
        "    pattern = re.compile(regex)\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        for bold in paragraph.find_all('b'):\n",
        "            return re.sub(pattern, \"\", paragraph.text)\n",
        "    \n",
        "print(get_first_paragraph(macron_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyZkffTEFR0z"
      },
      "source": [
        "Come up with other regexes to capture other patterns and sanitize the outputs completely. Modify your `get_first_paragraph()` function accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "NOmkLM9JFRCp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Emmanuel Macron, né le 21 décembre 1977 à Amiens est un haut fonctionnaire et homme d'État français. Il est président de la République française depuis 2017.\n",
            "Влади́мир Влади́мирович Пу́тин (род. 7 октября 1952, Ленинград, СССР)— российский государственный и политический деятель. Действующий президент Российской Федерации, председатель Государственного Совета Российской Федерации и верховный главнокомандующий Вооружёнными силами Российской Федерации с 7мая 2012 года. Ранее занимал должность президента с 7мая 2000года по 7мая 2008 года, также в 1999—2000 и 2008—2012 годах занимал должность председателя правительства Российской Федерации. Кандидат экономических наук (1997).\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def get_first_paragraph(url):\n",
        "    \n",
        "    regex_list = [\"\\ *\\(/.+/\\[e\\].*\\)\" , \n",
        "                  \"\\ *\\(/.+/.*\\)\",\n",
        "                  \"\\[[0-9]+\\]\" , \n",
        "                  \"[\\n]\", \n",
        "                  \"[\\t]\", \n",
        "                  \"[\\xa0]\"]\n",
        "    req = requests.get(url=url)\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "\n",
        "    for paragraph in soup.find_all('p'):\n",
        "\n",
        "        for bold in paragraph.find_all('b'):\n",
        "\n",
        "            text = paragraph.text\n",
        "\n",
        "            for regex in regex_list:\n",
        "\n",
        "                pattern = re.compile(regex)\n",
        "                text = re.sub(pattern, \"\", text)\n",
        "\n",
        "            return text\n",
        "\n",
        "def get_all_paragraphs(url):\n",
        "\n",
        "    req = requests.get(url=url)\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "    regex_list = [\"\\ *\\(/.+/\\[e\\].*\\)\" , \n",
        "                  \"\\ *\\(/.+/.*\\)\",\n",
        "                  \"\\[[0-9]+\\]\" , \n",
        "                  \"[\\n]\", \n",
        "                  \"[\\t]\", \n",
        "                  \"[\\xa0]\"]\n",
        "    paragraphs = []\n",
        "\n",
        "    for paragraph in soup.find_all('p'):\n",
        "\n",
        "        text = paragraph.text\n",
        "\n",
        "        for regex in regex_list:\n",
        "\n",
        "            pattern = re.compile(regex)\n",
        "            text = re.sub(pattern, \"\", text)\n",
        "            \n",
        "        paragraphs.append(text)\n",
        "\n",
        "    return paragraphs\n",
        "\n",
        "print(get_first_paragraph(macron_url))\n",
        "print(get_first_paragraph(putin_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9XUdX38V9XT"
      },
      "source": [
        "### Caching\n",
        "Good job! This function will be called many times. First because there are many leaders to loop through. Second because you are developing your code and are likely to re-run your loop multiple times. Third because the API might change and contain new leaders and you might want to update your data periodically.\n",
        "\n",
        "For all these reasons, it can be benificial to *cache* the result of the `get_first_paragraph()` function so that if it called with the same parameter, you expect the same output and thus do not need to query Wikipedia another time.\n",
        "\n",
        "You will use the [lru_cache](https://docs.python.org/3/library/functools.html#functools.lru_cache) decorator from the `functools` module. It is a tiny code change that will speed things up big time. You can use a `maxsize` of None. If you have questions about decorators, you will find more information [here](../../2.python/2.python_advanced/07.Decorator/decorators.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "qZfm-vFDXqi0"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import re\n",
        "\n",
        "#use lru cache to speedup the code\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def get_first_paragraph(url):\n",
        "    \n",
        "    regex_list = [\"\\ *\\(/.+/\\[e\\].*\\)\" , \n",
        "                  \"\\ *\\(/.+/.*\\)\",\n",
        "                  \"\\[[0-9]+\\]\" , \n",
        "                  \"[\\n]\", \n",
        "                  \"[\\t]\", \n",
        "                  \"[\\xa0]\"]\n",
        "    req = requests.get(url=url)\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "\n",
        "    for paragraph in soup.find_all('p'):\n",
        "\n",
        "        for bold in paragraph.find_all('b'):\n",
        "\n",
        "            text = paragraph.text\n",
        "\n",
        "            for regex in regex_list:\n",
        "\n",
        "                pattern = re.compile(regex)\n",
        "                text = re.sub(pattern, \"\", text)\n",
        "\n",
        "            return text\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DlE7uy2YvWN"
      },
      "source": [
        "Let's check the speedup!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "U46cCemPYhiG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to get the first paragraph of wikipedia page:  1.1304445266723633\n",
            "Time to get the first paragraph of wikipedia page:  4.291534423828125e-05\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Just run the cell. \n",
        "\n",
        "start = time.time()\n",
        "get_first_paragraph(macron_url)\n",
        "end = time.time()\n",
        "print(\"Time to get the first paragraph of wikipedia page: \", end - start)\n",
        "\n",
        "start = time.time()\n",
        "get_first_paragraph(macron_url)\n",
        "end = time.time()\n",
        "print(\"Time to get the first paragraph of wikipedia page: \", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY7bM7Z1Nr0K"
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "Let's go back to your `get_leaders()` function and update it with an *inner* loop over each leader. You will query the url provided and extract the first paragraph using the `get_first_paragraph()` function you just finished. You will then update that `leader`'s dictionary and move on to the next one.\n",
        "\n",
        "Notice, the rest of the code should not change since you modify the leader's data one by one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Dx7kAGa6OoS5"
      },
      "outputs": [],
      "source": [
        "# Just ctrl c v to avoid rerun the entire notebook\n",
        "import functools\n",
        "import re\n",
        "import time\n",
        "import bs4\n",
        "import requests\n",
        "\n",
        "def get_leaders():\n",
        "\n",
        "    root_url = \"https://country-leaders.onrender.com\"\n",
        "    cookie_url = \"/cookie\"\n",
        "    country_url = \"/countries\"\n",
        "    leaders_url = \"/leaders\"\n",
        "\n",
        "    requ = requests.get(root_url + cookie_url)\n",
        "    cookies = requ.cookies\n",
        "\n",
        "    requ1 = requests.get(root_url + country_url, cookies=cookies)\n",
        "    countries = requ1.json()\n",
        "\n",
        "    lead = [requests.get(root_url + leaders_url, cookies=cookies, params={\"country\": country}).json() for country in countries]\n",
        "    \n",
        "    leaders_per_country = dict(map(lambda i,j : (i,j) , countries , lead)) \n",
        "\n",
        "    for country, leaders in leaders_per_country.items():\n",
        "\n",
        "        for leader in leaders:\n",
        "\n",
        "            try:\n",
        "                leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = get_first_paragraph(leader[\"wikipedia_url\"])\n",
        "                \n",
        "            except Exception as e:\n",
        "                leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = \"Error\"\n",
        "                print(str(e))\n",
        "                continue\n",
        "\n",
        "    return leaders_per_country\n",
        "\n",
        "def get_first_paragraph(url):\n",
        "    \n",
        "    regex_list = [\"\\ *\\(/.+/\\[e\\].*\\)\" , \n",
        "                  \"\\ *\\(/.+/.*\\)\",\n",
        "                  \"\\[[0-9]+\\]\" , \n",
        "                  \"[\\n]\", \n",
        "                  \"[\\t]\", \n",
        "                  \"[\\xa0]\"]\n",
        "    req = requests.get(url=url)\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "\n",
        "    for paragraph in soup.find_all('p'):\n",
        "\n",
        "        for bold in paragraph.find_all('b'):\n",
        "\n",
        "            text = paragraph.text\n",
        "\n",
        "            for regex in regex_list:\n",
        "\n",
        "                pattern = re.compile(regex)\n",
        "                text = re.sub(pattern, \"\", text)\n",
        "\n",
        "            return text\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "RICG4T3DPZ1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'Q6279', 'first_name': 'Joe', 'last_name': 'Biden', 'birth_date': '1942-11-20', 'death_date': None, 'place_of_birth': \"St. Mary's Hospital\", 'wikipedia_url': 'https://en.wikipedia.org/wiki/Joe_Biden', 'start_mandate': '2021-01-20', 'end_mandate': None, 'first_paragraph': 'Joseph Robinette Biden Jr. is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama, and represented Delaware in the United States Senate from 1973 to 2009.'}\n"
          ]
        }
      ],
      "source": [
        "# Check the output of your function (2 lines)\n",
        "leaders_per_country = get_leaders()\n",
        "print(leaders_per_country[\"us\"][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sCKxGrnJCxv"
      },
      "source": [
        "Does the function crash in the middle of the loop? Chances are the cookies have expired while looping over the leaders.\n",
        "\n",
        "Modify your function to check if the `status_code` is a cookie error. In which case, get new ones and query the api again.\n",
        "\n",
        "If not, you may have a fast connection or a bit of luck. Do it anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "rgPd2dxgJiW1"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import re\n",
        "import time\n",
        "import bs4\n",
        "import requests\n",
        "import warnings\n",
        "from urllib3.exceptions import NewConnectionError\n",
        "\n",
        "\n",
        "def status_to_text(status_code : int) -> str:\n",
        "    status_dict = {\n",
        "        # infos response\n",
        "        100: \"Continue\",\n",
        "        101: \"Switching Protocols\",\n",
        "        102: \"Processing\",\n",
        "        103: \"Early Hints\",\n",
        "        # success response\n",
        "        200: \"OK\",\n",
        "        201: \"Created\",\n",
        "        202: \"Accepted\",\n",
        "        203: \"Non-Authoritative Information\",\n",
        "        204: \"No Content\",\n",
        "        205: \"Reset Content\",\n",
        "        206: \"Partial Content\",\n",
        "        207: \"Multi-Status\",\n",
        "        208: \"Already Reported\",\n",
        "        226: \"IM Used\",\n",
        "        # redirection response\n",
        "        300: \"Multiple Choices\",\n",
        "        301: \"Moved Permanently\",\n",
        "        302: \"Found\",\n",
        "        303: \"See Other\",\n",
        "        304: \"Not Modified\",\n",
        "        305: \"Use Proxy\",\n",
        "        306: \"Switch Proxy\",\n",
        "        307: \"Temporary Redirect\",\n",
        "        308: \"Permanent Redirect\",\n",
        "        # client error response\n",
        "        400: \"Bad Request\",\n",
        "        401: \"Unauthorized\",\n",
        "        402: \"Payment Required\",\n",
        "        403: \"Forbidden\",\n",
        "        404: \"Not Found\",\n",
        "        405: \"Method Not Allowed\",\n",
        "        406: \"Not Acceptable\",\n",
        "        407: \"Proxy Authentication Required\",\n",
        "        408: \"Request Timeout\",\n",
        "        409: \"Conflict\",\n",
        "        410: \"Gone\",\n",
        "        411: \"Length Required\",\n",
        "        412: \"Precondition Failed\",\n",
        "        413: \"Payload Too Large\",\n",
        "        414: \"URI Too Long\",\n",
        "        415: \"Unsupported Media Type\",\n",
        "        416: \"Range Not Satisfiable\",\n",
        "        417: \"Expectation Failed\",\n",
        "        418: \"I'm a teapot\",\n",
        "        421: \"Misdirected Request\",\n",
        "        422: \"Unprocessable Entity\",\n",
        "        423: \"Locked\",\n",
        "        424: \"Failed Dependency\",\n",
        "        425: \"Too Early\",\n",
        "        426: \"Upgrade Required\",\n",
        "        428: \"Precondition Required\",\n",
        "        429: \"Too Many Requests\",\n",
        "        431: \"Request Header Fields Too Large\",\n",
        "        451: \"Unavailable For Legal Reasons\",\n",
        "        # server error response\n",
        "        500: \"Internal Server Error\",\n",
        "        501: \"Not Implemented\",\n",
        "        502: \"Bad Gateway\",\n",
        "        503: \"Service Unavailable\",\n",
        "        504: \"Gateway Timeout\",\n",
        "        505: \"HTTP Version Not Supported\",\n",
        "        506: \"Variant Also Negotiates\",\n",
        "        507: \"Insufficient Storage\",\n",
        "        508: \"Loop Detected\",\n",
        "        510: \"Not Extended\",\n",
        "        511: \"Network Authentication Required\"\n",
        "    }\n",
        "    # find if the status code is in the keys\n",
        "    if status_code in status_dict.keys():\n",
        "        return str(status_code) + \": \" + status_dict[status_code]\n",
        "    else:\n",
        "        return str(status_code) + \": Unknown or unofficial status code\"\n",
        "\n",
        "\n",
        "def get_leaders():\n",
        "\n",
        "    root_url = \"https://country-leaders.onrender.com\"\n",
        "    cookie_url = \"/cookie\"\n",
        "    country_url = \"/countries\"\n",
        "    leaders_url = \"/leaders\"\n",
        "\n",
        "    requ = requests.get(root_url + cookie_url)\n",
        "    cookies = requ.cookies\n",
        "\n",
        "    requ1 = requests.get(root_url + country_url, cookies=cookies)\n",
        "    countries = requ1.json()\n",
        "\n",
        "    lead = [requests.get(root_url + leaders_url, cookies=cookies, params={\"country\": country}).json() for country in countries]\n",
        "    \n",
        "    leaders_per_country = dict(map(lambda i,j : (i,j) , countries , lead)) \n",
        "\n",
        "    for country, leaders in leaders_per_country.items():\n",
        "\n",
        "        for leader in leaders:\n",
        "\n",
        "            try:\n",
        "                leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = get_first_paragraph(leader[\"wikipedia_url\"])\n",
        "\n",
        "            except NewConnectionError as nce:\n",
        "                leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = \"NewConnectionError\" + str(nce)\n",
        "                print(str(nce))\n",
        "                continue\n",
        "\n",
        "            except Exception as e:\n",
        "                leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = \"Error\" + str(nce)\n",
        "                print(str(e))\n",
        "                continue\n",
        "\n",
        "    return leaders_per_country\n",
        "\n",
        "\n",
        "def get_first_paragraph(url):\n",
        "    \n",
        "    regex_list = [\"\\ *\\(/.+/\\[e\\].*\\)\" , \n",
        "                  \"\\ *\\(/.+/.*\\)\",\n",
        "                  \"\\[[0-9]+\\]\" , \n",
        "                  \"[\\n]\", \n",
        "                  \"[\\t]\", \n",
        "                  \"[\\xa0]\"]\n",
        "\n",
        "    req = requests.get(url=url)\n",
        "    \n",
        "    # print the content of the webpage if stautus is 200\n",
        "    if req.status_code != 200:\n",
        "        \n",
        "        message = root_url + status_url + \" \" + status_to_text(req.status_code)\n",
        "        warnings.warn(message)\n",
        "        return message\n",
        "\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "\n",
        "    for paragraph in soup.find_all('p'):\n",
        "\n",
        "        for bold in paragraph.find_all('b'):\n",
        "\n",
        "            text = paragraph.text\n",
        "\n",
        "            for regex in regex_list:\n",
        "\n",
        "                pattern = re.compile(regex)\n",
        "                text = re.sub(pattern, \"\", text)\n",
        "\n",
        "            return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JvV-kPsKLl0"
      },
      "source": [
        "Check the output of your function again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "mPXT-cxgKQof"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to scrap:  83.40544772148132\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Check the output of your function (1 line)\n",
        "\n",
        "start = time.time()\n",
        "leaders_per_country = get_leaders()\n",
        "end = time.time()\n",
        "print(\"Time to scrap: \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'Q6279', 'first_name': 'Joe', 'last_name': 'Biden', 'birth_date': '1942-11-20', 'death_date': None, 'place_of_birth': \"St. Mary's Hospital\", 'wikipedia_url': 'https://en.wikipedia.org/wiki/Joe_Biden', 'start_mandate': '2021-01-20', 'end_mandate': None, 'first_paragraph': 'Joseph Robinette Biden Jr. is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama, and represented Delaware in the United States Senate from 1973 to 2009.'}\n"
          ]
        }
      ],
      "source": [
        "print(leaders_per_country[\"us\"][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9lgxh_NMSO3"
      },
      "source": [
        "Well done! It took a while however... Let's speed things up. The main *bottleneck* is the loop. We call on the Wikipedia website many times.\n",
        "\n",
        "You will use the same *session* to call all the wikipedia pages. Check the *Advanced Usage* section of the Requests module's documentation.\n",
        "\n",
        "Start by modifying the `get_first_paragraph()` function to accept a session parameter and adjust the `get()` method call.\n",
        "Modify your `get_leaders()` function to make use of a single session for all the Wikipedia calls.\n",
        "1. create a `Session` object outside of the loop over countries.\n",
        "2. pass it to the `get_first_paragraph()` function as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "XTEVLIJ-V7z1"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import re\n",
        "import time\n",
        "import bs4\n",
        "import requests\n",
        "import warnings\n",
        "from requests import Session\n",
        "from urllib3.exceptions import NewConnectionError\n",
        "\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def status_to_text(status_code : int) -> str:\n",
        "    status_dict = {\n",
        "        # infos response\n",
        "        100: \"Continue\",\n",
        "        101: \"Switching Protocols\",\n",
        "        102: \"Processing\",\n",
        "        103: \"Early Hints\",\n",
        "        # success response\n",
        "        200: \"OK\",\n",
        "        201: \"Created\",\n",
        "        202: \"Accepted\",\n",
        "        203: \"Non-Authoritative Information\",\n",
        "        204: \"No Content\",\n",
        "        205: \"Reset Content\",\n",
        "        206: \"Partial Content\",\n",
        "        207: \"Multi-Status\",\n",
        "        208: \"Already Reported\",\n",
        "        226: \"IM Used\",\n",
        "        # redirection response\n",
        "        300: \"Multiple Choices\",\n",
        "        301: \"Moved Permanently\",\n",
        "        302: \"Found\",\n",
        "        303: \"See Other\",\n",
        "        304: \"Not Modified\",\n",
        "        305: \"Use Proxy\",\n",
        "        306: \"Switch Proxy\",\n",
        "        307: \"Temporary Redirect\",\n",
        "        308: \"Permanent Redirect\",\n",
        "        # client error response\n",
        "        400: \"Bad Request\",\n",
        "        401: \"Unauthorized\",\n",
        "        402: \"Payment Required\",\n",
        "        403: \"Forbidden\",\n",
        "        404: \"Not Found\",\n",
        "        405: \"Method Not Allowed\",\n",
        "        406: \"Not Acceptable\",\n",
        "        407: \"Proxy Authentication Required\",\n",
        "        408: \"Request Timeout\",\n",
        "        409: \"Conflict\",\n",
        "        410: \"Gone\",\n",
        "        411: \"Length Required\",\n",
        "        412: \"Precondition Failed\",\n",
        "        413: \"Payload Too Large\",\n",
        "        414: \"URI Too Long\",\n",
        "        415: \"Unsupported Media Type\",\n",
        "        416: \"Range Not Satisfiable\",\n",
        "        417: \"Expectation Failed\",\n",
        "        418: \"I'm a teapot\",\n",
        "        421: \"Misdirected Request\",\n",
        "        422: \"Unprocessable Entity\",\n",
        "        423: \"Locked\",\n",
        "        424: \"Failed Dependency\",\n",
        "        425: \"Too Early\",\n",
        "        426: \"Upgrade Required\",\n",
        "        428: \"Precondition Required\",\n",
        "        429: \"Too Many Requests\",\n",
        "        431: \"Request Header Fields Too Large\",\n",
        "        451: \"Unavailable For Legal Reasons\",\n",
        "        # server error response\n",
        "        500: \"Internal Server Error\",\n",
        "        501: \"Not Implemented\",\n",
        "        502: \"Bad Gateway\",\n",
        "        503: \"Service Unavailable\",\n",
        "        504: \"Gateway Timeout\",\n",
        "        505: \"HTTP Version Not Supported\",\n",
        "        506: \"Variant Also Negotiates\",\n",
        "        507: \"Insufficient Storage\",\n",
        "        508: \"Loop Detected\",\n",
        "        510: \"Not Extended\",\n",
        "        511: \"Network Authentication Required\"\n",
        "    }\n",
        "    # find if the status code is in the keys\n",
        "    if status_code in status_dict.keys():\n",
        "        return str(status_code) + \": \" + status_dict[status_code]\n",
        "    else:\n",
        "        return str(status_code) + \": Unknown or unofficial status code\"\n",
        "\n",
        "\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def get_lead(sess):\n",
        "\n",
        "    root_url = \"https://country-leaders.onrender.com\"\n",
        "    cookie_url = \"/cookie\"\n",
        "    country_url = \"/countries\"\n",
        "    leaders_url = \"/leaders\"\n",
        "\n",
        "    requ = sess.get(root_url + cookie_url)\n",
        "    cookies = requ.cookies\n",
        "\n",
        "    requ1 = sess.get(root_url + country_url, cookies=cookies)\n",
        "    countries = requ1.json()\n",
        "\n",
        "    lead = [sess.get(root_url + leaders_url, \n",
        "                        cookies=cookies, \n",
        "                        params={\"country\": country}).json() for country in countries]\n",
        "\n",
        "    leaders_dict = dict(map(lambda i,j : (i,j) , countries , lead)) \n",
        "\n",
        "    return leaders_dict\n",
        "\n",
        "\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def get_first_paragraph(url,sess):\n",
        "    \n",
        "    regex_list = [\"\\ *\\(/.+/\\[e\\].*\\)\" , \n",
        "                  \"\\ *\\(/.+/.*\\)\",\n",
        "                  \"\\[[0-9]+\\]\" , \n",
        "                  \"[\\n]\", \n",
        "                  \"[\\t]\", \n",
        "                  \"[\\xa0]\"]\n",
        "\n",
        "    req = sess.get(url=url)\n",
        "    \n",
        "    # print the content of the webpage if stautus is 200\n",
        "    if req.status_code != 200:\n",
        "        message = root_url + status_url + \" \" + status_to_text(req.status_code)\n",
        "        warnings.warn(message)\n",
        "        return message\n",
        "\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "\n",
        "    for paragraph in soup.find_all('p'):\n",
        "\n",
        "        for bold in paragraph.find_all('b'):\n",
        "\n",
        "            text = paragraph.text\n",
        "\n",
        "            for regex in regex_list:\n",
        "\n",
        "                pattern = re.compile(regex)\n",
        "                text = re.sub(pattern, \"\", text)\n",
        "\n",
        "            return text\n",
        "            \n",
        "\n",
        "def get_leaders():\n",
        "    with Session() as session:\n",
        "        \n",
        "        leaders_per_country = get_lead(session)\n",
        "\n",
        "        for country, leaders in leaders_per_country.items():\n",
        "\n",
        "            for leader in leaders:\n",
        "\n",
        "                try:\n",
        "                    leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = get_first_paragraph(leader[\"wikipedia_url\"],session)\n",
        "\n",
        "                except NewConnectionError as nce:\n",
        "                    leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = \"NewConnectionError\" + str(nce)\n",
        "                    print(str(nce))\n",
        "                    continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = \"Error\" + str(nce)\n",
        "                    print(str(e))\n",
        "                    continue\n",
        "            \n",
        "    return leaders_per_country\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy09nMG9VOaI"
      },
      "source": [
        "Test your new functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "p5wFE6ivRf-Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to scrap:  55.0427930355072\n"
          ]
        }
      ],
      "source": [
        "# Just run it.\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "leaders_per_country = get_leaders()\n",
        "end = time.time()\n",
        "print(\"Time to scrap: \", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVA5raJxVEtr"
      },
      "source": [
        "Using a Session makes the first call to `get_leaders()` faster however, for technical reasons, the `@lru_cache` decorator no longer works. \n",
        "\n",
        "Problem: The `session` parameter is not *hashable* and since the cache is a dictionary using all the parameters as key, this poses a problem.\n",
        "\n",
        "Solution: Create our own cache that only cares about the url.\n",
        "\n",
        "The code for this is provided. You should only read it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VcbiLxfaVFOq"
      },
      "outputs": [],
      "source": [
        "cache = {}\n",
        "def hashable_cache(f):\n",
        "    def inner(url, session):\n",
        "        if url not in cache:\n",
        "            cache[url] = f(url, session)\n",
        "        return cache[url]\n",
        "    return inner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJU9Hw8jZbV8"
      },
      "source": [
        "Overwrite the `get_first_paragraph()` function to use the new decorator like this: `@hashable_cache`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "T6FbVRw8Z3kA"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import re\n",
        "import time\n",
        "import bs4\n",
        "import requests\n",
        "import warnings\n",
        "from urllib3.exceptions import NewConnectionError\n",
        "\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def status_to_text(status_code : int) -> str:\n",
        "    status_dict = {\n",
        "        # infos response\n",
        "        100: \"Continue\",\n",
        "        101: \"Switching Protocols\",\n",
        "        102: \"Processing\",\n",
        "        103: \"Early Hints\",\n",
        "        # success response\n",
        "        200: \"OK\",\n",
        "        201: \"Created\",\n",
        "        202: \"Accepted\",\n",
        "        203: \"Non-Authoritative Information\",\n",
        "        204: \"No Content\",\n",
        "        205: \"Reset Content\",\n",
        "        206: \"Partial Content\",\n",
        "        207: \"Multi-Status\",\n",
        "        208: \"Already Reported\",\n",
        "        226: \"IM Used\",\n",
        "        # redirection response\n",
        "        300: \"Multiple Choices\",\n",
        "        301: \"Moved Permanently\",\n",
        "        302: \"Found\",\n",
        "        303: \"See Other\",\n",
        "        304: \"Not Modified\",\n",
        "        305: \"Use Proxy\",\n",
        "        306: \"Switch Proxy\",\n",
        "        307: \"Temporary Redirect\",\n",
        "        308: \"Permanent Redirect\",\n",
        "        # client error response\n",
        "        400: \"Bad Request\",\n",
        "        401: \"Unauthorized\",\n",
        "        402: \"Payment Required\",\n",
        "        403: \"Forbidden\",\n",
        "        404: \"Not Found\",\n",
        "        405: \"Method Not Allowed\",\n",
        "        406: \"Not Acceptable\",\n",
        "        407: \"Proxy Authentication Required\",\n",
        "        408: \"Request Timeout\",\n",
        "        409: \"Conflict\",\n",
        "        410: \"Gone\",\n",
        "        411: \"Length Required\",\n",
        "        412: \"Precondition Failed\",\n",
        "        413: \"Payload Too Large\",\n",
        "        414: \"URI Too Long\",\n",
        "        415: \"Unsupported Media Type\",\n",
        "        416: \"Range Not Satisfiable\",\n",
        "        417: \"Expectation Failed\",\n",
        "        418: \"I'm a teapot\",\n",
        "        421: \"Misdirected Request\",\n",
        "        422: \"Unprocessable Entity\",\n",
        "        423: \"Locked\",\n",
        "        424: \"Failed Dependency\",\n",
        "        425: \"Too Early\",\n",
        "        426: \"Upgrade Required\",\n",
        "        428: \"Precondition Required\",\n",
        "        429: \"Too Many Requests\",\n",
        "        431: \"Request Header Fields Too Large\",\n",
        "        451: \"Unavailable For Legal Reasons\",\n",
        "        # server error response\n",
        "        500: \"Internal Server Error\",\n",
        "        501: \"Not Implemented\",\n",
        "        502: \"Bad Gateway\",\n",
        "        503: \"Service Unavailable\",\n",
        "        504: \"Gateway Timeout\",\n",
        "        505: \"HTTP Version Not Supported\",\n",
        "        506: \"Variant Also Negotiates\",\n",
        "        507: \"Insufficient Storage\",\n",
        "        508: \"Loop Detected\",\n",
        "        510: \"Not Extended\",\n",
        "        511: \"Network Authentication Required\"\n",
        "    }\n",
        "    # find if the status code is in the keys\n",
        "    if status_code in status_dict.keys():\n",
        "        return str(status_code) + \": \" + status_dict[status_code]\n",
        "    else:\n",
        "        return str(status_code) + \": Unknown or unofficial status code\"\n",
        "\n",
        "\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def get_lead(sess):\n",
        "\n",
        "    root_url = \"https://country-leaders.onrender.com\"\n",
        "    cookie_url = \"/cookie\"\n",
        "    country_url = \"/countries\"\n",
        "    leaders_url = \"/leaders\"\n",
        "\n",
        "    requ = sess.get(root_url + cookie_url)\n",
        "    cookies = requ.cookies\n",
        "\n",
        "    requ1 = sess.get(root_url + country_url, cookies=cookies)\n",
        "    countries = requ1.json()\n",
        "\n",
        "    lead = [sess.get(root_url + leaders_url, \n",
        "                        cookies=cookies, \n",
        "                        params={\"country\": country}).json() for country in countries]\n",
        "\n",
        "    leaders_dict = dict(map(lambda i,j : (i,j) , countries , lead)) \n",
        "\n",
        "    return leaders_dict\n",
        "\n",
        "\n",
        "@hashable_cache\n",
        "def get_first_paragraph(url,sess):\n",
        "    \n",
        "    regex_list = [\"\\ *\\(/.+/\\[e\\].*\\)\" , \n",
        "                  \"\\ *\\(/.+/.*\\)\",\n",
        "                  \"\\[[0-9]+\\]\" , \n",
        "                  \"[\\n]\", \n",
        "                  \"[\\t]\", \n",
        "                  \"[\\xa0]\"]\n",
        "\n",
        "    req = sess.get(url=url)\n",
        "    \n",
        "    # print the content of the webpage if stautus is 200\n",
        "    if req.status_code != 200:\n",
        "        message = root_url + status_url + \" \" + status_to_text(req.status_code)\n",
        "        warnings.warn(message)\n",
        "        return message\n",
        "\n",
        "    soup = bs4.BeautifulSoup(req.text, \"html.parser\")\n",
        "\n",
        "    for paragraph in soup.find_all('p'):\n",
        "\n",
        "        for bold in paragraph.find_all('b'):\n",
        "\n",
        "            text = paragraph.text\n",
        "\n",
        "            for regex in regex_list:\n",
        "\n",
        "                pattern = re.compile(regex)\n",
        "                text = re.sub(pattern, \"\", text)\n",
        "\n",
        "            return text\n",
        "            \n",
        "\n",
        "def get_leaders():\n",
        "    with Session() as session:\n",
        "        \n",
        "        leaders_per_country = get_lead(session)\n",
        "\n",
        "        for country, leaders in leaders_per_country.items():\n",
        "\n",
        "            for leader in leaders:\n",
        "\n",
        "                try:\n",
        "                    leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = get_first_paragraph(leader[\"wikipedia_url\"],session)\n",
        "\n",
        "                except NewConnectionError as nce:\n",
        "                    leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = \"NewConnectionError\" + str(nce)\n",
        "                    print(str(nce))\n",
        "                    continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    leaders_per_country[country][leaders.index(leader)][\"first_paragraph\"] = \"Error\" + str(nce)\n",
        "                    print(str(e))\n",
        "                    continue\n",
        "            \n",
        "    return leaders_per_country\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LypJpspWZ8se"
      },
      "source": [
        "Check that the cache is working again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "Zd9uYmq0aEQa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to scrap:  1.059046983718872\n"
          ]
        }
      ],
      "source": [
        "# Just run it.\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "leaders_per_country = get_leaders()\n",
        "end = time.time()\n",
        "print(\"Time to scrap: \", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtLTvw3wPqe0"
      },
      "source": [
        "## Saving your hard work\n",
        "\n",
        "The final step is to save the ``leaders_per_country`` dictionary in the `leaders.json` file using the [json](https://docs.python.org/3/library/json.html) module. Make a tour in our [file handling section](../../2.python/2.python_advanced/04.File-handling/file_handling.ipynb) to know more about how to read and write files using Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "pTNGKKrOjNDk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"leaders.json\", \"w\") as the_file:\n",
        "    json.dump(leaders_per_country, the_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7uf_kfGCWmM"
      },
      "source": [
        "Make sure the file can be read back. Write the code to read the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "4VwNjBYyjPzs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "George Washington (February 22, 1732[b]– December 14, 1799) was an American military officer, statesman, and Founding Father who served as the first president of the United States from 1789 to 1797. Appointed by the Continental Congress as commander of the Continental Army, Washington led the Patriot forces to victory in the American Revolutionary War and served as the president of the Constitutional Convention of 1787, which created the Constitution of the United States and the American federal government. Washington has been called the \"Father of his Country\" for his manifold leadership in the formative days of the country.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open(\"leaders.json\", \"r\") as the_file:\n",
        "    leaders_per_country = json.load(the_file)\n",
        "print(leaders_per_country[list(leaders_per_country.keys())[0]][0][\"first_paragraph\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fW_U7gXktyv"
      },
      "source": [
        "Make a function `save(leaders_per_country)` to call this code easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "EfknpnTljqUd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def save(leaders_per_country):\n",
        "    with open(\"leaders.json\", \"w\") as the_file:\n",
        "        json.dump(leaders_per_country, the_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "lWQ6bbn31cix"
      },
      "outputs": [],
      "source": [
        "# Call the function (1 line)\n",
        "save(leaders_per_country)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5W6nPihlQo9"
      },
      "source": [
        "## Tidy things up in a stand-alone python script\n",
        "\n",
        "Congratulations! You now have a working scraper! However, your code is scattered throughout this notebook along side the tutorials. Hardly production ready...\n",
        "\n",
        "Copy and paste what you need in a separate `leaders_scraper.py` file.\n",
        "Make sure it works by calling `python3 leaders_scraper.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0cvv193mlxY"
      },
      "source": [
        "## To go further\n",
        "\n",
        "If you want to practice scraping, you can read this section and tackle the exercise.\n",
        "\n",
        "You have noticed the API returns very partial results for country leaders. Many are missing. Overwrite the `get_leaders()` function to get its list from Wikipedia and extract their *personal details* from the frame on the side.\n",
        "\n",
        "Good luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
